{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def simulation_demonstration(df_list, simulations, plot_original=False, plot_dummied=False, plot_transformed=False, plot_transformed_simulation = False, plot_back_transformed_simulation = False,return_diff=False, return_dfs=False):\n",
    "    '''The function that conducts the full transformation, simulation and back-transformation. As it was designed for a \n",
    "    specific experiment, the parameters \"df_list\" and \"simulations\" are intended to be lists of several dataframes and \n",
    "    simulation numbers to ease the use for single and multiple values for these parameters. \n",
    "    \n",
    "    The parameters starting with \"plot_\" give the plots of the datasets at different stages of the simulation. \n",
    "    \n",
    "    \"return_diff\" gives two dataframes that compares the absolute difference of the correlation matrices for each given \n",
    "    dataframe at diffferent stages of the simulation as well as for each number of simulation. Requires the dataframes\n",
    "    to have names assigned to them.\n",
    "    \n",
    "    \"return_dfs\" returns the final simulated dataframes.'''\n",
    "    \n",
    "    # Initialize lists to store the results\n",
    "    orr_trans=[]\n",
    "    trans_clean=[]\n",
    "    \n",
    "    orr_corr_sim=[]\n",
    "    orr_back_trans=[]\n",
    "    \n",
    "    total_full_sims=[]\n",
    "    \n",
    "    # Loop through the dataframes\n",
    "    for count, df in enumerate(df_list):\n",
    "        \n",
    "        column_order = df.columns\n",
    "\n",
    "        # Plot the original data\n",
    "        if plot_original==True :\n",
    "            print('Dataframe ' + str(count+1) + ' out of ' + str(len(df_list)) + ' Dataframes')\n",
    "            for col in df.columns:\n",
    "\n",
    "                # Check if the column is numeric\n",
    "                if pd.api.types.is_numeric_dtype(df[col]):\n",
    "\n",
    "                    # If it's numeric, plot a histogram\n",
    "                    sns.histplot(data=df, x=col)\n",
    "\n",
    "                else:\n",
    "                    # If it's not numeric, plot a countplot\n",
    "                    sns.countplot(data=df, x=col)\n",
    "\n",
    "                # Show the plot\n",
    "                plt.show()\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Perform one-hot encoding\n",
    "        enc_df=pd.get_dummies(df.select_dtypes('object'), drop_first=False, prefix_sep='___')\n",
    "        df_num = df.select_dtypes(exclude=\"object_\").join(enc_df)\n",
    "        \n",
    "        # Calculate the correlation matrix of the original data\n",
    "        corr_orig = df_num.corr()\n",
    "\n",
    "        # Plot the data after one-hot encoding\n",
    "        if plot_dummied==True:\n",
    "            print('Dataframe ' + str(count+1) + ' out of ' + str(len(df_list)) + ' Dataframes')\n",
    "            df_num.hist(figsize=(20, 20), bins=20, xlabelsize=8, ylabelsize=8)\n",
    "            plt.show()\n",
    "            continue\n",
    "\n",
    "        #power transform (and standardize), that is, make it more Gaussian \n",
    "        pt = PowerTransformer()\n",
    "        df_num_trans=pt.fit_transform(df_num)\n",
    "        df_num_trans = pd.DataFrame(data=df_num_trans, columns=df_num.columns)\n",
    "\n",
    "        \n",
    "        # Plot the data after power transformation\n",
    "        if plot_transformed==True:\n",
    "            print('Dataframe ' + str(count+1) + ' out of ' + str(len(df_list)) + ' Dataframes')\n",
    "            df_num_trans.hist(figsize=(20, 20), bins=20, xlabelsize=8, ylabelsize=8)\n",
    "            plt.show()\n",
    "            continue\n",
    "            \n",
    "        \n",
    "         # Calculate the correlation matrix of the transformed data\n",
    "        corr_trans = df_num_trans.corr()\n",
    "        \n",
    "        # Calculate the difference in correlation matrices before and after transformation\n",
    "        orr_trans.append(np.mean(np.absolute(np.subtract(corr_orig.values ,corr_trans.values))))\n",
    "\n",
    "        # Simulate random nubers between 0 and 1 for each value of simulations\n",
    "        x_uncor=[]\n",
    "        for i in simulations:\n",
    "            x_uncor.append(np.random.normal(0, 1, (len(df_num.columns), i)))\n",
    "\n",
    "        # Clean correlation matrix by adding a marginal number to the diagonal\n",
    "        # It works because the source of negative eigenvalues (incrementally close to 0) is so-called \"numeric fuzz\" that arises when using python float values, see:\n",
    "        # https://pythonnumericalmethods.berkeley.edu/notebooks/chapter09.03-Roundoff-Errors.html#:~:text=This%20has%20a%20side%20effect,is%20called%20round%2Doff%20error.\n",
    "        C = np.diag([0.1] * corr_trans.shape[0])\n",
    "        corr_trans_clean = corr_trans+C\n",
    "\n",
    "        # Calculate the difference in correlation matrices before and after adding the small number to the diagonal\n",
    "        trans_clean.append(np.mean(np.absolute(np.subtract(corr_trans_clean.values ,corr_trans.values))))\n",
    "\n",
    "        # Correlate monte carlo simulations and save datasets and their correlation matrices\n",
    "        L = np.linalg.cholesky(corr_trans_clean)\n",
    "        x_cor=[]\n",
    "        x_data=[]\n",
    "        for simul in x_uncor:\n",
    "            y = np.dot(L, simul)\n",
    "            y=pd.DataFrame(y).T\n",
    "            y.columns = df_num.columns\n",
    "            x_data.append(y)\n",
    "            y= y.corr()\n",
    "            x_cor.append(y)\n",
    "\n",
    "        # Plot the simulations after correlating them\n",
    "        if plot_transformed_simulation==True:\n",
    "            print('Dataframe ' + str(count+1) + ' out of ' + str(len(df_list)) + ' Dataframes')\n",
    "            for i, simul in zip(x_data, simulations):\n",
    "                print(str(simul) + 'Simulations')\n",
    "                i.hist(figsize=(20, 20), bins=20, xlabelsize=8, ylabelsize=8)\n",
    "                plt.show()\n",
    "                continue\n",
    "\n",
    "        # Check how close the simulated correlation matrices get to the original correlation matrix\n",
    "        for i in x_cor:\n",
    "            orr_corr_sim.append(np.mean(np.absolute(np.subtract(i.values ,corr_orig.values))))\n",
    "\n",
    "\n",
    "        # Transform simulated datasets back in order to get the original distribution (including un-standardizing)\n",
    "        x_data2=[]\n",
    "        for i in x_data:\n",
    "            back_transform= pt.inverse_transform(i)\n",
    "            back_transform = pd.DataFrame(back_transform, columns=df_num.columns)\n",
    "            x_data2.append(back_transform)\n",
    "\n",
    "        # Check differences in correlation between original correlation and simulated, back-transformed\n",
    "        for i in x_data2:\n",
    "            orr_back_trans.append(np.mean(np.absolute(np.subtract(i.corr().values ,corr_orig.values))))\n",
    "\n",
    "        #Plot back-transformed simulations\n",
    "        if plot_back_transformed_simulation==True:\n",
    "            print('Dataframe ' + str(count+1) + ' out of ' + str(len(df_list)) + ' Dataframes')\n",
    "            for i, simul in zip(x_data2, simulations):\n",
    "                print(str(simul) + 'Simulations')\n",
    "                i.hist(figsize=(20, 20), bins=20, xlabelsize=8, ylabelsize=8)\n",
    "                plt.show()\n",
    "                continue\n",
    "\n",
    "        # Initialize lists of column groups\n",
    "        dummy_groups = list(df.select_dtypes(\"object\").columns)\n",
    "        binaries=[]\n",
    "        numerics=[]\n",
    "\n",
    "        # Loop through all numeric column names and check if it is a binary or not\n",
    "        for col in df.select_dtypes(exclude=\"object_\").columns:\n",
    "\n",
    "            # Select the column and get its minimum and maximum values\n",
    "            min_value = df_num[col].min()\n",
    "            max_value = df_num[col].max()\n",
    "\n",
    "            # Check if the minimum and maximum values are equal to 0 and 1, respectively\n",
    "            if min_value == 0 and max_value == 1 and len(df_num[col].unique()) == 2:\n",
    "                # If so, append the column name to the list of selected columns\n",
    "                binaries.append(col)\n",
    "            else :\n",
    "                numerics.append(col)\n",
    "        # Initalize lists of sub-dataframes\n",
    "        df_non_cats=[]\n",
    "        df_rounded_dummies=[]\n",
    "        full_sims=[]\n",
    "\n",
    "        # Save mean values of binaries for proper rounding\n",
    "        mean_values=df[binaries].mean()\n",
    "\n",
    "\n",
    "        # Loop though correlated simulations\n",
    "        for i in x_data2:\n",
    "\n",
    "            # Fill missing values\n",
    "            for feature in i.columns:\n",
    "                i[feature].fillna(i[feature].mean(), inplace=True)\n",
    "\n",
    "            # Merge non-categorical columns together    \n",
    "            df_non_cat = pd.concat([i[numerics], i[binaries]], axis=1)\n",
    "\n",
    "            # Round binaries\n",
    "            for feature in df_non_cat[binaries]:\n",
    "                df_non_cat[feature] = (df_non_cat[feature] > mean_values[feature]).astype(int)\n",
    "\n",
    "            # Set highest number of each dummy per categorical column to 1 and rest to 0\n",
    "            empty= pd.DataFrame()\n",
    "            for cat in dummy_groups:\n",
    "                part_df= i.iloc[:, i.columns.str.startswith(cat)]\n",
    "                part_df_rounded = part_df.apply(lambda x: pd.Series([1 if i == x.max() else 0 for i in x], index=part_df.columns), axis=1)\n",
    "                empty = pd.concat([empty, part_df_rounded], axis=1)\n",
    "\n",
    "            # De-dummy categorical columns\n",
    "            cats_sim = pd.from_dummies(empty, sep='___')\n",
    "\n",
    "\n",
    "            # Merge non-categorical sub-data with categorical-data\n",
    "            full_sim = df_non_cat.join(cats_sim)\n",
    "            full_sim = full_sim.reindex(columns=column_order)\n",
    "            full_sims.append(full_sim)\n",
    "        \n",
    "        # Save full simulations in list    \n",
    "        total_full_sims.append(full_sims)\n",
    "\n",
    "\n",
    "    #Return dataframe with differences between correlation matrices\n",
    "    if return_diff == True:\n",
    "        df_names = ['Small', 'German', 'Deloitte', 'Large']  \n",
    "        # Create output dataframe\n",
    "        single_matrix_diffs = pd.DataFrame(\n",
    "        {'Datasets' : df_names,\n",
    "         'original vs. transformed': orr_trans,\n",
    "         'transformed vs. cleaned': trans_clean,})\n",
    "        single_matrix_diffs.set_index('Datasets', inplace = True)    \n",
    "\n",
    "        #Set MultiIndex to ensure proper comparison between datasets and simulations\n",
    "        index = pd.MultiIndex.from_product([simulations, df_names], names=['number of simulations', 'dataset'])\n",
    "\n",
    "        # Create output dataframe\n",
    "        sim_matrix_diffs = pd.DataFrame({'original vs. simulated & correlated': orr_corr_sim,\n",
    "                                         'simulated, correlated & back-transformed': orr_back_trans},\n",
    "                                         index=index)\n",
    "\n",
    "        return single_matrix_diffs, sim_matrix_diffs\n",
    "    \n",
    "    # Return final simulated dataframes as a list\n",
    "    if return_dfs == True:\n",
    "        return total_full_sims\n",
    "    \n",
    "def al_splitting(orr_df, full_sim):\n",
    "    '''perform splitting for Active Learning. \n",
    "    Inputs are:\n",
    "        orr_df: original dataset\n",
    "        full_sim: full Monte Carlo Simulation of the given dataframe\n",
    "        \n",
    "    Outputs are:\n",
    "        df_start: dataset of accepted cases for the start\n",
    "        df_al_splits: list of dataframes for active learning consisting of the original dataset\n",
    "        df_al_mc_splits: list of dataframes for active learning consisting of the original and simulated cases'''\n",
    "    simulated_bads = full_sim[full_sim['BAD'] == 1]\n",
    "\n",
    "    simulated_bads_sample = simulated_bads.sample(n=(round(0.2/0.8*0.9*len(orr_df))), random_state=888)\n",
    "    \n",
    "    # Split the dataframe into independent and dependent variables and shuffle\n",
    "    sets = train_test_split(orr_df, test_size=0.1, random_state=888) \n",
    "    df_al, df_start = sets\n",
    "\n",
    "    df_al_mc = pd.concat([df_al,simulated_bads_sample])\n",
    "\n",
    "\n",
    "    # Shuffle the Dataframe and convert it to an array\n",
    "#    df_al_array = df_al.sample(frac=1, random_state=888).values\n",
    "\n",
    "#    # Split the array into parts of specified size\n",
    "#    splits = np.array_split(df_al_array, 9)\n",
    "\n",
    "    # Convert each split back to a DataFrame and store it in a list\n",
    "#    df_al_splits = [pd.DataFrame(split, columns=orr_df.columns) for split in splits]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Shuffle the Dataframe and convert it to an array\n",
    "#    df_al_mc_array = df_al_mc.sample(frac=1, random_state=888).values\n",
    "    df_al_mc = df_al_mc.sample(frac=1, random_state=888)\n",
    "\n",
    "    # Split the array into parts of specified size\n",
    "#    splits = np.array_split(df_al_mc_array, 9)\n",
    "\n",
    "    # Convert each split back to a DataFrame and store it in a list\n",
    "#    df_al_mc_splits = [pd.DataFrame(split, columns=orr_df.columns) for split in splits]\n",
    "    \n",
    "    return df_start, df_al, df_al_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
